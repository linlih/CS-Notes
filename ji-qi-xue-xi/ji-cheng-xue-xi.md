# 集成学习

集成分类的分类：

| 类别 | 分类器 | 例子 |
| :--- | :--- | :--- |
| 同质\(homogeneous\) | 基学习器\(base learner\) | 全由决策树构成 |
| 异质\(heterogeneous\) | 组件学习器\(component learner\) 或个体学习器 | 由神经网络和决策树构成 |

集成学习首先要考虑的问题是，如何保证集成后的效果比没有集成的情况下要更好呢？因为有时候并不一定说把两个不好的集成在一起就好，两个好的东西集成在一起也一定好。

这里就涉及到集成时要判定的两个依据：一个是个体学习器要具备一定的**准确率**，另外一个是学习器之间具有**多样性**。

![](../.gitbook/assets/image%20%289%29.png)

集成学习实现方法

| 类别特点 | 例子 |
| :--- | :--- |
| 个体学习器间**存在强依赖**关系，必须串行生成的**序列化**方法 | Boosting |
| 个体学习器间**不存在强依赖**关系，可同时生成的**并行化**方法 | Bagging、随机森林 |

## Boosting

Boosting算法的核心思想是：从初始训练集中训练得到一个基学习器，然后根据基学习器对于样本预测的结果，让判别错误的样本在后续的训练中得到更多的关注。然后再基于调整过的样本训练下一个基学习器。反复这个过程，直到训练好T个学习器，最终对这些学习器进行加权结合。

下面我们看Boosting中最著名的算法代表：AdaBoost





|  | Boosting | Bagging |
| :--- | :--- | :--- |
| 关注点 | 降低偏差 | 降低方差 |
| 适用场景 | 泛化能力弱的学习器 | 不剪枝的决策树、神经网络等易受样本扰动的学习器 |





## 组合策略









































