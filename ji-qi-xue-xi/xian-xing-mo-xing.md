# 线性模型

## 推导过程：

首先我们从回归问题开始，从最简单的线性模型开始，到广义的线性模型。然后推导到如何解决分类问题。

## 什么叫线性模型

首先我们要看一个问题就是：什么叫线性模型？

线性表示的是变量与变量之间成比例关系，这个比例关系是一个常数，非线性自然就是二者不是成一个常数的比例关系。

但是这里我们看Logistic回归模型中，函数的表达式也明显变量间不是成比例关系，为什么还称之为线性模型呢？这是因为在这里我们对于模型线性的定义有所不同。依据的w的变化影响到的x有几个，如果只有一个，那么称之为线性模型，反之称之为非线性模型。

具体的描述，参阅参考文献。

## L1&L2正则

L1正则在统计学的文献中被称之为lasso（least absolute shrinkage and selection operator最小绝对收缩和选择算子）\([Tibshirani,1996](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)\)，表现为如果超参数$$\lambda$$充分大，那么某些系数$$w_j$$就会变为零，从而产生一个稀疏模型。产生这样的结果原因是什么呢？

直观的解释如下：假设现在有一个二维的$$w=(w_1, w_2)$$，蓝色是误差函数的优化，橙色是正则化的限制区域，从右图中可以看到，此时最优解$$w^*$$ 在$$w_2$$轴上，此时的$$w_1 = 0$$，更倾向得到稀疏解。左边的L2正则的限制区域为圆，此时得到的$$w^*$$落在象限内，两个维度上都不为0。

![](../.gitbook/assets/image%20%2813%29.png)

这里给出了关于L1为什么得到的是稀疏模型的更多[解释1](https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models)，[解释2](https://www.zhihu.com/question/37096933)，[解释3](https://blog.csdn.net/fantacy10000/article/details/90647686)

总结起来可按照三个方面来分析L1和L2正则：

1. 从优化目标函数的表达式进行求导得到梯度的不同进行分析
2. 从解空间，也就是上图
3. 从先验分布进行分析，L1相当于引入了拉普拉斯分布，而L2相当于引入了高斯分布

## 简单的线性回归问题

假设我们一个样本中有d个属性，那么我们可以依据对d个属性的线性组合来得到一个预测的函数：

$$
\begin{aligned}
y &= w_1x_1 + w_2x_2 +...+ w_dx_d\\
f(x)&=\boldsymbol {w^Tx}+b
\end{aligned}
$$

**这样的模型好处是什么呢？**

$$w$$向量可以很直观的看到每个属性对于预测的重要程度，比如说$$y=0.7x_1+0.2x_2+0.1x_3$$，可以得到$$x_1$$是一个非常重要的属性，基本上对输出的结果起着决定性的作用。

现在我们来看下如何求解上面式子中的$$w,b$$。

这就涉及到我们需要确定优化的对象，单单看上面这个式子，我们是没有办法解出来的。我们知道这条曲线是为了拟合我们的样本数据的，所以就要求拟合的曲线尽量和样本点越接近越好，理想状态下是拟合的曲线都是经过样本点。那么如何衡量当下我们的曲线和样本的差别呢，我们可以考虑$$f(x)$$和$$y$$的差值，这里$$f(x)$$指的是我们模型的输出结果，$$y$$是样本的实际值。

引入均方误差作为性能度量：

$$
\begin{aligned}(w^*,b^*)&= argmin_{w,b}\sum^{m}_{i=1}(f(x_i)-y_i)^2\\
&=argmin_{w,b}\sum_{i=1}^m(y_i - wx_i - b)^2
\end{aligned}
$$

根据这个要优化的式子，我们可以对这个表达式进行求导：

$$
\frac{\partial E}{\partial w} = 2(w\sum_{i=1}^{m}x_i^2-\sum_{i=1}^m(y_i-b)x_i) =0\\
\frac{\partial E}{\partial b} = 2(mb-\sum_{i=1}^m(y_i-wx_i))=0\\
$$

最终得到：

$$
w=\frac{\sum \limits _{i=1}^{m}y_i(x_i-\overline{x})}{\sum \limits _{i=1}^{m}x_i^2-\frac{1}{m}(\sum \limits _{i=1}^{m}x_i)^2}\\
b=\frac{1}{m}\sum \limits _{i=1}^{m}(y_i-wx_i)
$$

ok！至此我们就可以根据样本信息，得到了最后的拟合曲线了。

上诉的求解方法我们称之为最小二乘法，那还有其他方法吗？





### 参考文献

* \[1\]谨慎殷勤.怎样区分线性和非线性\_线性与非线性的区别.[CSDN](https://blog.csdn.net/weixin_41797870/article/details/85012811)
* \[2\]wikipedia.[Lasso \(statistics\)](https://en.wikipedia.org/wiki/Lasso_%28statistics%29).















