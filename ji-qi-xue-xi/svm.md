# SVM

### 拉格朗日函数

### 

### 

### SVM原理

首先需要知道支持向量机的分类：

1. 线性可分支持向量机

   这种向量机是属于最为理想的状态，满足两个条件，线性和可分。

2. 线性支持向量机

   这种向量机则允许出现一些错误的数据点

3. 非线性支持向量机

   这种向量机就是能够处理非线性的数据，通过特征映射的方式将非线性的数据信息转化为线性可分的数据，然后再利用向量机进行分类。

简单的理解表现在二维平面上就是线性的分类方式为一条直线，而非线性的分类方式为一条曲线。

#### 函数间隔与几何间隔：

#### 参数说明

线性核：

C参数越来越大，分类的界面越来越窄，表明使用了越来越少的样本来做分类，有可能导致过拟合，但是C值越大，能使得训练精度变高。实际中需要确认想要的目标是训练精度高，还是泛化能力强的问题

高斯核：

γ足够大的话，分界曲面靠近于训练数据，换句话说，总是可以将γ值取得足够大的话，使得它的分类准确度是百分之一百

当γ足够小的时候，高斯核就退化成为了线性核 当γ足够大的时候，高斯核就退化成为了K近邻

#### 线性分类

假设在一个平面中，存在两个类别的样本，并且是可分的，那么一定可以找到无数条直线将其分开。存在一条直线可以将其分开，并且离样本的距离最大。

假设这条直线为：2x + y - 4 = 0

那么我们可以写成\(2,1\)·\(x, y\) - 4 = 0

即将\(2, 1\)作为w，\(x, y\)作为x，-4作为b，这样分界面就可以写成wx + b = 0

这里我们可以看到w和x为向量，b为标量

此时将样本点带入wx + b中得到的结果如果为正，样本点就位于直线法线的正向，反之则位于法线的负向。

那么此时如何衡量样本属于正例还是负例呢？这里使用的是距离的方式来衡量，\|wx + b\| / \|w\|就可以得到样本到分界面wx + b = 0的距离。如果该样本属于正例的话，那么wx + b的结果为一个正数，如果是负例的话，则是一个负数，那么这个时候如果我们给wx + b乘上标记值y，那就可以去掉wx + b的绝对值符号了，结果为y\*\(wx + b\) / \|w\|, 这就是最终的距离衡量式子。

那么我们要的目标是什么呢？

我们希望的是能够找到一个分割面，二维的话就是一条直线，能够使得样本离这条直线的距离最大。那么在训练过程中，我们需要保证的是，所有样本中最小的那个距离达到最大，这样整个的距离就达到了最大。

目标函数：argmax min

上面的目标函数中：有这一个1/\|w\|项，这个\|w\|的计算是这样的，假设它为k维，那么w12+w22+w32+...+wk2再开根号，如果直接用这个来进行优化，非常复杂麻烦，所以需要做一个等价的变化。

比如说刚刚的直线：2x + y - 4 = 0，那么我们总是可以计算所有的样本到这条直线的距离，那么必然存在最小距离，我们可以将这个距离进行缩放到1，这个是可以做到的，原因是这样，假设直线的最小距离算出来为7，那么我们对直线距离计算公式两边同时除以7，那么就可以让距离变为1，那么此时的直线可能转化为 2/4 x + 1/4 y - 4 / 4 = 0。那么这个时候y\*\(wx + b\)这个项就为1，就可以去掉了。

那么目标函数就可以化简为：argmax 1/\|w\|

此时我们多了一个约束条件，就是y\*\(wx + b\)要保证大于等于1，为什么呢？因为我们上面是求的argmax min\(y \* \(wx + b\)\),那么这个时候如果y\*\(wx + b\)满足大于等于1的话，那么它取最最小值就是1了

要求1/\|w\|最大，那就是要求\|w\|最小，要求\|w\|最小，就相当于求1/2\|w\|2最小，其中的1/2和平方都是为了后面的求解方便。

那么现在怎么做呢？带条件的最优化问题，需要用到拉格朗日乘子法

**需要补充KKT条件，对偶函数**

#### 线性支持向量机

若数据线性不可分，或者存在一个样本点使得最终的结果泛化能力降低，那么可以增加松弛因子来解决这个问题。

约束条件则转化为：y\*\(wx + b\) &gt;= 1 - 松弛因子

目标函数则转化为：min 1/2 \|w\| 2 + C\*松弛因子

这里的C为一个超参数，保证》0

实际上最终我们得到的松弛因子就是损失，前面的是1/2 \|w\| 2是一个L2的正则项

SVM存在的问题：

1. 训练样本不能太大，否则训练速度很慢
2. 
SVM的优点有：

1. SVM本身基本一定的防止过拟合的能力

SVM可以用来划分多分类吗？

可以，两个方法，一是直接多分类，时间复杂度是K方乘以一个SVM的时间复杂度和1 vs 1的分类器是一样的，二是使用1 vs rest / 1 vs 1

神经网络也可以处理非线性问题，为什么去全连接层换成SVM的效果会更好呢？在这个问题上SVM会有优势吗？

SVM会更好一点，因为它天然地加入了一点防止过拟合的东西，可能会好一些

一个softmax回归就是一个神经网络

为了减轻噪声样本对SVM的影响，那么就把C调小，使得泛化能力提高



### 参考文献

* \[1\]秦曾昌.机器学习算法精讲.小象学院，2018.



