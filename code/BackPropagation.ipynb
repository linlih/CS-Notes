{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropagation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBAEjPAmK2PHq+eVzeiC3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linlih/CS-Notes/blob/master/code/BackPropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3nphHdbhnO",
        "colab_type": "text"
      },
      "source": [
        "# 反向传播代码示例\n",
        "\n",
        "参考自：https://www.cnblogs.com/charlotte77/p/5629865.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9xvZIvXIRla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klPoC0YXIpO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "参数说明：\n",
        "\"pd_\":偏导的前缀，partial derivative\n",
        "\"d_\" :倒数的前缀，derivative\n",
        "\"w_ho\": 隐含层到输出层的权重系数索引\n",
        "\"w_ih\": 输出层到隐含层的权重系数索引\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUioNachLIRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  LEARING_RATE = 0.5\n",
        "  \n",
        "  def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
        "    self.num_inputs = num_inputs\n",
        "    self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
        "    self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
        "    self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
        "    self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
        "  \n",
        "  def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
        "    weight_num = 0\n",
        "    for h in range(len(self.hidden_layer.neurons)):\n",
        "      for i in range(self.num_inputs):\n",
        "        if not hidden_layer_weights:\n",
        "          self.hidden_layer.neurons[h].weights.append(random.random())\n",
        "        else:\n",
        "          self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
        "          weight_num += 1\n",
        "        \n",
        "  def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
        "    weight_num = 0\n",
        "    for o in range(len(self.output_layer.neurons)):\n",
        "      for h in range(len(self.hidden_layer.neurons)):\n",
        "        if not output_layer_weights:\n",
        "          self.output_layer.neurons[o].weights.append(random.random())\n",
        "        else:\n",
        "          self.output_layer.neurons[o].weights.append(output_layer_weights[weightnum])\n",
        "          weight_num += 1\n",
        "  \n",
        "  def inspect(self):\n",
        "    print('-----------------')\n",
        "    print(\"Input:{}\".format(self.num_input))\n",
        "    print('-----------------')\n",
        "    print(\"Hidden Layer\")\n",
        "    self.hidden_layer.inspect()\n",
        "    print('-----------------')\n",
        "    print(\"Output Layer\")\n",
        "    self.output_layer.inspect()\n",
        "    print('-----------------')\n",
        "\n",
        "  def feed_forward(self, inputs):\n",
        "    hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
        "    return self.output_layer.feed_forward(hidden_layer_outputs)\n",
        "\n",
        "  def train(self, training_inputs, training_outputs):\n",
        "    self.feed_forward(training_inputs)\n",
        "  \n",
        "    # 1.输出神经元的值\n",
        "    pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
        "    for o in range(len(self.output_layer.neurons)):\n",
        "      # ∂E/∂zⱼ\n",
        "      pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
        "    \n",
        "    # 2.隐含层神经元的值\n",
        "    pd_error_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
        "    for h in range(len(self.hidden_layer.neurons)):\n",
        "\n",
        "      # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
        "      d_error_wrt_hidden_neuron_output = 0\n",
        "      for o in range(len(self.output_layer.neurons)):\n",
        "        d_error_wrt_hidden_neuron_output += pd_error_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
        "        # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
        "        pd_errors_wrt_output_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
        "\n",
        "    # 3.更新输出层权重系数\n",
        "    for o in range(len(self.output_layer.neurons)):\n",
        "      for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
        "        pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
        "\n",
        "        self.output_layer.neurons[o].weights[w_ho] -= self.LEARING_RATE * pd_error_wrt_weight\n",
        "    \n",
        "    # 4.更新隐含层的权重系数\n",
        "    for h in range(len(self.hidden_layer.neurons)):\n",
        "      for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
        "        pd_error_wrt_weight = pd_error_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih);\n",
        "        self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARING_RATE * pd_error_wrt_weight\n",
        "  \n",
        "  def calculate_total_error(self, training_sets):\n",
        "    total_error = 0\n",
        "    for t in range(len(training_sets)):\n",
        "      training_inputs, training_outputs = training_sets[t]\n",
        "      self.feed_forward(training_inputs)\n",
        "      for o in range(len(training_outputs[o])):\n",
        "        total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
        "    return total_error\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ruHYqDfVZ-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD1nPuSfLfao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}